apiVersion: v1
kind: Service
metadata:
  name: upstream-vllm-a100-40gb-service
spec:
  type: LoadBalancer
  selector:
    app: upstream-vllm-a100-40gb
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000
---
apiVersion: v1
kind: Pod
metadata:
  name: upstream-vllm-a100-40gb
  labels:
    app: upstream-vllm-a100-40gb
spec:
  nodeSelector:
    nodetype: gpu
    gpuspecs: A100_40GB
  tolerations:
    - effect: NoSchedule
      operator: Equal
      value: "gpu"
      key: "nodetype"
  restartPolicy: Never
  containers:
    - name: ubuntu
      tty: true
      image: vllm/vllm-openai:latest
      imagePullPolicy: Always
      command:
        [
          "python3",
          "-m",
          "vllm.entrypoints.openai.api_server",
          "--model",
          "NousResearch/Llama-2-7b-chat-hf",
          "--host",
          "0.0.0.0",
          "--port",
          "8000",
        ]
      resources:
        limits:
          memory: "40Gi"
          cpu: "12"
          nvidia.com/gpu: 1
      env:
        - name: HF_HOME
          value: "/cache"
      ports:
        - containerPort: 8000
      volumeMounts:
        - name: external-volume
          mountPath: "/cache"
        - name: dshm
          mountPath: "/dev/shm"
        - name: shared-network-drive
          mountPath: "/network"
  volumes:
    - name: external-volume
      hostPath:
        path: "/cache"
    - name: dshm
      emptyDir:
        medium: Memory
    - name: shared-network-drive
      nfs:
        server: storage-gateway.wisp.internal.neuralmagic.com
        path: /on-prem-storage-gw
